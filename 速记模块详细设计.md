# 速记模块详细设计文档

## 版本与依赖

```
# requirements.txt — 核心依赖
langchain==1.2.7
langchain-core==1.2.0
langchain-openai==1.1.3
langchain-neo4j==0.4.0
langgraph==0.4.x
fastapi==0.115.x
uvicorn==0.34.x
sqlalchemy==2.0.x
alembic==1.14.x
celery[redis]==5.4.x
redis==5.2.x
neo4j==5.27.x
pydantic==2.10.x
faster-whisper==1.1.x
python-multipart==0.0.x
```

Python 版本要求：3.11+（langchain 1.2.x 最低要求 3.10，推荐 3.11 以获得更好的类型支持）。

## 一、速记模块的核心概念

速记模块处理两种内容类型，它们在数据流转、Agent 决策逻辑上有本质区别。

### 1.1 速记 (QuickNote)

速记是用户随手记录的碎片化信息。特点是短、频率高、上下文不完整。处理策略是"被动匹配"——基于已有的知识图谱结构，把新内容挂到合适的位置。

典型场景：
- "今天学了 LangGraph 的 StateGraph 用法，比之前的 AgentExecutor 灵活很多"
- "和老王聊了一下明年的预算方案，他觉得云服务成本可以砍 30%"
- "跑步 5 公里 用时 28 分钟"

### 1.2 事件 (Event)

事件是用户需要达成的项目、长期任务、重大影响事件、或者用户的强性格标记。特点是持续时间长、影响范围广、会被反复引用。处理策略是"主动搜索"——用 Agent 全面扫描现有内容，找出所有可能关联的速记和其他事件。

典型场景：
- "Q1 完成知识图谱项目上线"（项目类事件）
- "每周至少跑步 3 次"（长期习惯类事件）
- "公司组织架构调整，技术部拆分为平台和业务两个团队"（重大影响类事件）
- "我是一个对代码质量有强迫症的人"（性格标记类事件）

### 1.3 两者的本质差异

| 维度 | 速记 QuickNote | 事件 Event |
|------|---------------|-----------|
| 生命周期 | 一次性写入，偶尔回顾 | 持续活跃，反复关联 |
| 关联方向 | 速记 → 找到归属（被动） | 事件 → 吸引内容（主动） |
| Agent 策略 | 先查现有图谱结构，匹配分类/标签/事件 | 全量检索用户内容，主动建立关联 |
| 处理时效 | 实时（用户等待 1-3 秒） | 异步（后台 Celery 执行，10-30 秒） |
| 图谱操作 | 轻量写入：1个Memo节点 + 若干关系边 | 重量写入：1个Event节点 + 批量关系边 |

## 二、数据流转全景

```
用户输入（文字 / 语音转文字）
    │
    ▼
┌──────────────────────────────────┐
│  FastAPI 接口层                   │
│  POST /api/v1/memos              │
│  POST /api/v1/events             │
│  POST /api/v1/memos/audio        │
└───────────────┬──────────────────┘
                │
    ┌───────────┴───────────┐
    │                       │
    ▼                       ▼
同步处理（速记）         异步处理（事件）
    │                       │
    ▼                       ▼
┌──────────┐          ┌──────────────┐
│ MySQL    │          │ Celery Task  │
│ 写入原文  │          │ 异步执行     │
└────┬─────┘          └──────┬───────┘
     │                       │
     ▼                       ▼
┌───────────────────────────────────┐
│       LangGraph Agent 工作流       │
│                                    │
│  ┌─────────┐  速记流程              │
│  │ 查询现有 │──► 匹配分类 ──► 提取  │
│  │ 图谱结构 │    & 标签     实体    │
│  └─────────┘       │        │      │
│                    ▼        ▼      │
│              查询相关内容 ──► 关联  │
│              (基于实体/标签)  判定   │
│                              │     │
│  ┌─────────┐  事件流程       │     │
│  │ 深度分析 │──► 全量向量 ──►│     │
│  │ 事件语义 │    检索         │     │
│  └─────────┘       │        │     │
│                    ▼        │     │
│              图谱多跳遍历 ──►│     │
│                              │     │
│                    ┌─────────▼──┐  │
│                    │ 写入 Neo4j  │  │
│                    └────────────┘  │
└───────────────────────────────────┘
```

## 三、Agent 设计（核心）

### 3.1 整体 Agent 架构

不使用简单的线性 chain，而是基于 LangGraph 构建有状态的多节点工作流。两种内容类型共享部分节点，但走不同的分支路径。

```python
# memo_agent/state.py
from typing import Annotated, TypedDict
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages


class MemoProcessState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]

    user_id: int
    memo_id: int
    memo_type: str                     # "quick_note" | "event"
    content: str                       # 原始内容
    title: str

    # ---- 以下字段由各节点逐步填充 ----
    user_graph_context: dict           # 用户现有的图谱结构（分类树、常用标签等）
    classification_result: dict        # 分类结果
    extraction_result: dict            # 实体 & 标签提取结果
    relation_candidates: list[dict]    # 候选关联内容
    final_relations: list[dict]        # 最终确认的关联关系
    event_links: list[dict]            # 绑定的事件列表
    neo4j_operations: list[dict]       # 待执行的 Neo4j 写入操作
```

### 3.2 Pydantic 结构化输出模型

所有与 LLM 交互的环节都通过 `with_structured_output` 强制结构化，避免 JSON 解析错误。

```python
# memo_agent/schemas.py
from pydantic import BaseModel, Field


class ClassificationResult(BaseModel):
    """速记/事件的分类结果"""
    primary_category: str = Field(description="一级分类名称，必须从用户已有分类中选择")
    secondary_category: str = Field(description="二级分类名称，必须从用户已有分类中选择")
    confidence: float = Field(ge=0, le=1, description="分类置信度")
    reasoning: str = Field(description="分类理由，简要说明为什么选择这个分类")
    suggest_new_category: str | None = Field(
        default=None,
        description="如果现有分类都不合适，建议创建的新分类名称"
    )


class ExtractedTag(BaseModel):
    """提取的标签"""
    name: str = Field(description="标签名称")
    is_existing: bool = Field(description="是否是用户已有的标签")
    relevance: float = Field(ge=0, le=1, description="与内容的相关度")


class ExtractedEntity(BaseModel):
    """提取的实体"""
    name: str = Field(description="实体名称")
    entity_type: str = Field(description="实体类型：Person/Organization/Technology/Location/Concept/Project")
    properties: dict = Field(default_factory=dict, description="实体的附加属性")


class ExtractionResult(BaseModel):
    """实体与标签的提取结果"""
    tags: list[ExtractedTag] = Field(description="提取的标签列表，优先复用已有标签")
    entities: list[ExtractedEntity] = Field(description="提取的实体列表")
    summary: str = Field(description="内容的一句话摘要")
    time_reference: str | None = Field(default=None, description="内容中提及的时间信息")


class RelationJudgment(BaseModel):
    """关联关系的判定结果"""
    target_id: int = Field(description="关联目标的 memo_id 或 event_id")
    target_type: str = Field(description="目标类型：memo 或 event")
    relation_type: str = Field(description="关系类型：RELATED_TO / LINKED_TO / CONTRADICTS / EXTENDS / CAUSED_BY")
    score: float = Field(ge=0, le=1, description="关联强度评分")
    reason: str = Field(description="关联原因的简要说明")
    should_link: bool = Field(description="是否应该建立关联")


class RelationBatchResult(BaseModel):
    """批量关联判定结果"""
    judgments: list[RelationJudgment] = Field(description="每一条候选内容的关联判定")
    overall_analysis: str = Field(description="对整体关联情况的分析总结")


class EventAnalysis(BaseModel):
    """事件的深度分析结果"""
    event_type: str = Field(description="事件类型：project/habit/impact/personality/milestone")
    keywords: list[str] = Field(description="事件的核心关键词，用于后续检索")
    time_scope: str | None = Field(default=None, description="事件的时间范围描述")
    expected_related_topics: list[str] = Field(
        description="预期可能关联的主题方向，Agent 将据此主动搜索"
    )
    priority: str = Field(description="事件优先级：high/medium/low")
    is_recurring: bool = Field(description="是否是周期性/持续性事件")


class EventBindingDecision(BaseModel):
    """将速记绑定到事件的决策结果"""
    event_id: int = Field(description="要绑定的事件 ID")
    event_title: str = Field(description="事件标题")
    binding_reason: str = Field(description="绑定理由")
    binding_strength: float = Field(ge=0, le=1, description="绑定强度")
    should_bind: bool = Field(description="是否应该绑定")


class EventBindingBatchResult(BaseModel):
    """批量事件绑定决策"""
    decisions: list[EventBindingDecision]
    auto_detected_events: list[str] = Field(
        default_factory=list,
        description="在内容中检测到但用户尚未创建的潜在事件"
    )
```

### 3.3 LangGraph 工作流节点实现

#### 节点 1：加载用户图谱上下文

```python
# memo_agent/nodes/load_context.py
from neo4j import AsyncGraphDatabase

NEO4J_URI = "bolt://localhost:7687"
NEO4J_AUTH = ("neo4j", "password")


async def load_user_graph_context(state: MemoProcessState) -> dict:
    """
    加载用户的图谱上下文信息，供后续节点的 LLM prompt 使用。
    包括：分类树、常用标签 Top30、活跃事件列表、最近实体。
    这一步不调用 LLM，纯粹是 Neo4j 查询。
    """
    user_id = state["user_id"]
    
    async with AsyncGraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH) as driver:
        async with driver.session() as session:
            # 查询 1：获取用户的分类树
            categories_result = await session.run("""
                MATCH (u:User {user_id: $uid})-[:PREFERS]->(c1:Category)
                OPTIONAL MATCH (c2:Category)-[:CHILD_OF]->(c1)
                RETURN c1.name AS primary, collect(c2.name) AS secondaries
            """, uid=user_id)
            categories = {}
            async for record in categories_result:
                categories[record["primary"]] = record["secondaries"]

            # 查询 2：获取用户常用标签 Top 30
            tags_result = await session.run("""
                MATCH (u:User {user_id: $uid})-[:OWNS]->(m)-[:HAS_TAG]->(t:Tag)
                RETURN t.name AS tag, count(m) AS usage_count
                ORDER BY usage_count DESC LIMIT 30
            """, uid=user_id)
            frequent_tags = []
            async for record in tags_result:
                frequent_tags.append({
                    "name": record["tag"],
                    "count": record["usage_count"]
                })

            # 查询 3：获取活跃事件列表
            events_result = await session.run("""
                MATCH (u:User {user_id: $uid})-[:OWNS]->(e:Event)
                WHERE e.status IN ['active', 'in_progress']
                RETURN e.event_id AS id, e.title AS title,
                       e.description AS desc, e.event_type AS type,
                       e.keywords AS keywords
                ORDER BY e.updated_at DESC LIMIT 20
            """, uid=user_id)
            active_events = []
            async for record in events_result:
                active_events.append(dict(record))

            # 查询 4：最近 7 天的实体
            entities_result = await session.run("""
                MATCH (u:User {user_id: $uid})-[:OWNS]->(m)-[:MENTIONS]->(en:Entity)
                WHERE m.created_at > datetime() - duration('P7D')
                RETURN DISTINCT en.name AS name, en.type AS type
                LIMIT 50
            """, uid=user_id)
            recent_entities = []
            async for record in entities_result:
                recent_entities.append(dict(record))

    return {
        "user_graph_context": {
            "categories": categories,
            "frequent_tags": frequent_tags,
            "active_events": active_events,
            "recent_entities": recent_entities,
        }
    }
```

#### 节点 2：分类

```python
# memo_agent/nodes/classify.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from memo_agent.schemas import ClassificationResult


async def classify_node(state: MemoProcessState) -> dict:
    """
    根据用户现有的分类体系，为新内容匹配最合适的分类。
    如果所有分类都不合适，会建议创建新分类。
    """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    structured_llm = llm.with_structured_output(ClassificationResult)

    ctx = state["user_graph_context"]
    category_tree_str = "\n".join([
        f"  {primary}: [{', '.join(subs)}]"
        for primary, subs in ctx["categories"].items()
    ])

    prompt = ChatPromptTemplate.from_messages([
        ("system", """你是一个内容分类专家。用户有如下分类体系：

{category_tree}

请将用户的新内容分配到最合适的一级和二级分类中。

规则：
1. 必须从已有分类中选择，除非确实没有合适的分类
2. 如果内容跨多个分类，选择最核心的那个
3. 置信度低于 0.5 时，必须填写 suggest_new_category
4. 内容类型是 "{memo_type}"，事件类的内容倾向于归入与目标/任务相关的分类"""),
        ("human", "请分类以下内容：\n\n标题：{title}\n内容：{content}")
    ])

    chain = prompt | structured_llm
    result = await chain.ainvoke({
        "category_tree": category_tree_str,
        "memo_type": state["memo_type"],
        "title": state["title"],
        "content": state["content"],
    })

    return {"classification_result": result.model_dump()}
```

#### 节点 3：提取标签与实体

```python
# memo_agent/nodes/extract.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from memo_agent.schemas import ExtractionResult


async def extract_tags_entities_node(state: MemoProcessState) -> dict:
    """
    从内容中提取标签和实体。
    标签优先复用用户已有标签，实体按类型（人物/组织/技术/地点/概念/项目）归类。
    """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    structured_llm = llm.with_structured_output(ExtractionResult)

    ctx = state["user_graph_context"]
    existing_tags_str = ", ".join([t["name"] for t in ctx["frequent_tags"]])
    recent_entities_str = ", ".join([
        f"{e['name']}({e['type']})" for e in ctx["recent_entities"]
    ])

    prompt = ChatPromptTemplate.from_messages([
        ("system", """你是一个信息提取专家。从用户的速记内容中提取标签和实体。

用户已有的高频标签（优先复用）：
{existing_tags}

用户最近提及的实体（注意去重和合并同义词）：
{recent_entities}

规则：
1. 标签数量控制在 3-8 个，优先使用已有标签
2. 如果内容中的概念与已有标签语义接近，使用已有标签而非创建新标签
3. 实体类型固定为：Person / Organization / Technology / Location / Concept / Project
4. 一句话摘要不超过 30 字
5. 如果内容中有明确的时间信息（日期、时间段），请提取到 time_reference"""),
        ("human", "请提取以下内容的标签和实体：\n\n{content}")
    ])

    chain = prompt | structured_llm
    result = await chain.ainvoke({
        "existing_tags": existing_tags_str,
        "recent_entities": recent_entities_str,
        "content": state["content"],
    })

    return {"extraction_result": result.model_dump()}
```

#### 节点 4A：速记的关联查找（被动匹配）

速记走被动匹配路线。用提取出的实体和标签去 Neo4j 查询已有的相关内容，然后让 LLM 判断是否建立关联。

```python
# memo_agent/nodes/find_relations_quicknote.py
from neo4j import AsyncGraphDatabase

from memo_agent.state import MemoProcessState

NEO4J_URI = "bolt://localhost:7687"
NEO4J_AUTH = ("neo4j", "password")


async def find_relations_quicknote(state: MemoProcessState) -> dict:
    """
    基于提取出的标签和实体，在 Neo4j 中查找可能相关的内容。
    这是"被动匹配"策略——不做全量扫描，只查关联路径上的内容。
    """
    user_id = state["user_id"]
    extraction = state["extraction_result"]
    tag_names = [t["name"] for t in extraction["tags"]]
    entity_names = [e["name"] for e in extraction["entities"]]

    candidates = []

    async with AsyncGraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH) as driver:
        async with driver.session() as session:
            # 策略 1：通过共同标签查找（标签路径）
            if tag_names:
                result = await session.run("""
                    MATCH (u:User {user_id: $uid})-[:OWNS]->(m)-[:HAS_TAG]->(t:Tag)
                    WHERE t.name IN $tags AND m.memo_id <> $current_id
                    WITH m, collect(t.name) AS shared_tags, count(t) AS tag_overlap
                    ORDER BY tag_overlap DESC
                    LIMIT 10
                    RETURN m.memo_id AS id, m.title AS title,
                           m.content AS content, m.type AS type,
                           shared_tags, tag_overlap,
                           'tag_path' AS source
                """, uid=user_id, tags=tag_names, current_id=state["memo_id"])
                async for record in result:
                    candidates.append(dict(record))

            # 策略 2：通过共同实体查找（实体路径）
            if entity_names:
                result = await session.run("""
                    MATCH (u:User {user_id: $uid})-[:OWNS]->(m)-[:MENTIONS]->(en:Entity)
                    WHERE en.name IN $entities AND m.memo_id <> $current_id
                    WITH m, collect(en.name) AS shared_entities, count(en) AS entity_overlap
                    ORDER BY entity_overlap DESC
                    LIMIT 10
                    RETURN m.memo_id AS id, m.title AS title,
                           m.content AS content, m.type AS type,
                           shared_entities, entity_overlap,
                           'entity_path' AS source
                """, uid=user_id, entities=entity_names, current_id=state["memo_id"])
                async for record in result:
                    candidates.append(dict(record))

            # 策略 3：通过同分类查找最近的内容（分类路径）
            classification = state["classification_result"]
            result = await session.run("""
                MATCH (u:User {user_id: $uid})-[:OWNS]->(m)-[:BELONGS_TO]->(c:Category {name: $cat})
                WHERE m.memo_id <> $current_id
                WITH m ORDER BY m.created_at DESC LIMIT 5
                RETURN m.memo_id AS id, m.title AS title,
                       m.content AS content, m.type AS type,
                       'category_path' AS source
            """, uid=user_id,
                cat=classification["secondary_category"],
                current_id=state["memo_id"])
            async for record in result:
                candidates.append(dict(record))

    # 去重（同一个 memo 可能通过多条路径被找到，保留路径信息）
    seen_ids = set()
    unique_candidates = []
    for c in candidates:
        if c["id"] not in seen_ids:
            seen_ids.add(c["id"])
            unique_candidates.append(c)

    return {"relation_candidates": unique_candidates}
```

#### 节点 4B：事件的关联查找（主动搜索）— ReAct Agent

事件走主动搜索路线，使用 ReAct Agent 自主决定搜索策略。这是整个系统最核心的 Agent。

```python
# memo_agent/nodes/find_relations_event.py
import json

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from neo4j import AsyncGraphDatabase
from typing import Annotated, TypedDict

from memo_agent.schemas import EventAnalysis
from memo_agent.state import MemoProcessState

NEO4J_URI = "bolt://localhost:7687"
NEO4J_AUTH = ("neo4j", "password")


# ============================================================
# 事件 Agent 的工具集定义
# ============================================================

@tool
async def vector_search_memos(query: str, user_id: int, top_k: int = 15) -> str:
    """
    通过语义向量相似度搜索用户的速记和事件。
    适用于：模糊语义匹配，比如搜索"关于成本优化的讨论"。
    参数：
        query: 搜索查询文本
        user_id: 用户 ID
        top_k: 返回结果数量
    """
    async with AsyncGraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH) as driver:
        async with driver.session() as session:
            # 先获取 query 的 embedding（实际中通过 embedding model）
            from langchain_openai import OpenAIEmbeddings
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            query_vector = await embeddings.aembed_query(query)

            result = await session.run("""
                CALL db.index.vector.queryNodes('memo_embeddings', $top_k, $vector)
                YIELD node, score
                WHERE node.user_id = $uid
                RETURN node.memo_id AS id, node.title AS title,
                       left(node.content, 200) AS content_preview,
                       labels(node)[0] AS type, score
                ORDER BY score DESC
            """, top_k=top_k, vector=query_vector, uid=user_id)
            
            items = []
            async for record in result:
                items.append(dict(record))
            return json.dumps(items, ensure_ascii=False)


@tool
async def search_by_entity_graph(entity_name: str, user_id: int, hops: int = 2) -> str:
    """
    通过实体名称在知识图谱中进行多跳搜索。
    从一个实体出发，沿关系边遍历，找出 N 跳内的所有相关内容。
    适用于：找出与某个人/组织/技术相关的所有笔记。
    参数：
        entity_name: 实体名称（人名、组织名、技术名等）
        user_id: 用户 ID
        hops: 最大跳数（1-3）
    """
    async with AsyncGraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH) as driver:
        async with driver.session() as session:
            result = await session.run("""
                MATCH (en:Entity)
                WHERE en.name CONTAINS $name
                WITH en LIMIT 3
                CALL {
                    WITH en
                    MATCH path = (en)<-[:MENTIONS]-(m)<-[:OWNS]-(u:User {user_id: $uid})
                    RETURN m.memo_id AS id, m.title AS title,
                           left(m.content, 200) AS content_preview,
                           labels(m)[0] AS type,
                           1 AS distance
                    UNION
                    WITH en
                    MATCH (en)-[:RELATED_TO]-(en2:Entity)<-[:MENTIONS]-(m)<-[:OWNS]-(u:User {user_id: $uid})
                    RETURN m.memo_id AS id, m.title AS title,
                           left(m.content, 200) AS content_preview,
                           labels(m)[0] AS type,
                           2 AS distance
                }
                RETURN DISTINCT id, title, content_preview, type, min(distance) AS distance
                ORDER BY distance ASC
                LIMIT 20
            """, name=entity_name, uid=user_id)
            
            items = []
            async for record in result:
                items.append(dict(record))
            return json.dumps(items, ensure_ascii=False)


@tool
async def search_by_tags(tag_names: list[str], user_id: int) -> str:
    """
    通过标签组合搜索内容。找出包含指定标签（任意匹配）的速记和事件。
    适用于：通过主题标签定位相关内容。
    参数：
        tag_names: 标签名称列表
        user_id: 用户 ID
    """ 
    async with AsyncGraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH) as driver:
        async with driver.session() as session:
            result = await session.run("""
                MATCH (u:User {user_id: $uid})-[:OWNS]->(m)-[:HAS_TAG]->(t:Tag)
                WHERE t.name IN $tags
                WITH m, collect(t.name) AS matched_tags
                RETURN m.memo_id AS id, m.title AS title,
                       left(m.content, 200) AS content_preview,
                       labels(m)[0] AS type,
                       matched_tags
                ORDER BY size(matched_tags) DESC
                LIMIT 20
            """, tags=tag_names, uid=user_id)
            
            items = []
            async for record in result:
                items.append(dict(record))
            return json.dumps(items, ensure_ascii=False)


@tool  
async def search_by_time_range(start_date: str, end_date: str, user_id: int) -> str:
    """
    通过时间范围搜索速记和事件。
    适用于：找出某个时间段内的所有内容，用于时间线关联。
    参数：
        start_date: 开始日期（格式：YYYY-MM-DD）
        end_date: 结束日期
        user_id: 用户 ID
    """
    async with AsyncGraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH) as driver:
        async with driver.session() as session:
            result = await session.run("""
                MATCH (u:User {user_id: $uid})-[:OWNS]->(x)
                WHERE x.created_at >= date($start) AND x.created_at <= date($end)
                RETURN coalesce(x.memo_id, x.event_id) AS id,
                       coalesce(x.title, 'Event') AS title,
                       labels(x)[0] AS type,
                       coalesce(x.content, x.description) AS content,
                       x.created_at AS created_at
                ORDER BY created_at DESC
                LIMIT 30
            """, start=start_date, end=end_date, uid=user_id)
            
            items = []
            async for record in result:
                items.append(dict(record))
            return json.dumps(items, ensure_ascii=False)


# ============================================================
# 事件关联 Agent 的状态定义
# ============================================================

class EventRelationAgentState(TypedDict):
    messages: Annotated[list, add_messages]
    user_id: int
    event_id: int
    event_data: dict  # event 的标题、描述等
    event_analysis: EventAnalysis
    collected_candidates: list[dict]
    final_decisions: list[dict]


# ============================================================
# 事件关联 Agent 节点定义
# ============================================================

async def event_agent_call_model(state: EventRelationAgentState):
    """事件关联 Agent 的 LLM 调用节点"""
    llm = ChatOpenAI(model="gpt-4o")
    tools = [vector_search_memos, search_by_entity_graph, search_by_tags, search_by_time_range]
    bound_llm = llm.bind_tools(tools)
    
    system_msg = SystemMessage(content="""
你是一个智能内容关联专家。用户刚刚创建了一个新的事件，你的任务是：

1. 首先深度分析这个事件：
   - 识别事件类型（project/habit/impact/personality/milestone）
   - 提取核心关键词
   - 判断时间范围
   - 推测可能关联的主题方向

2. 然后主动搜索用户的所有速记和事件：
   - 使用 vector_search_memos 进行语义模糊匹配
   - 使用 search_by_entity_graph 找到与实体相关的内容
   - 使用 search_by_tags 通过主题查找
   - 使用 search_by_time_range 按时间范围查找

3. 最后判断哪些内容应该与该事件建立关联：
   - 评估每个候选内容的关联度
   - 决定关系类型（RELATED_TO / LINKED_TO / EXTENDS / CONTRADICTS）
   - 给出关联理由

重要原则：
- 宁可漏掉一些弱关联，不要建立无意义的强关联
- 优先考虑语义相关性，而不是字面匹配
- 对于事件类内容，要特别关注时间上的连续性
- 每次工具调用都要说明你的搜索意图
- 不要重复搜索相同的内容
- 搜索完成后，输出最终的关联决策列表

输出格式要求：
对于每个最终确认的关联，输出：
```json
{{
  "target_id": 123,
  "target_type": "memo",
  "relation_type": "RELATED_TO",
  "score": 0.85,
  "reason": "该速记记录了事件初期的重要决策",
  "should_link": true
}}
```
""")
    
    response = bound_llm.invoke([system_msg] + state["messages"])
    return {"messages": [response]}


async def event_agent_tool_node(state: EventRelationAgentState):
    """事件关联 Agent 的工具执行节点"""
    tools_by_name = {
        "vector_search_memos": vector_search_memos,
        "search_by_entity_graph": search_by_entity_graph,
        "search_by_tags": search_by_tags,
        "search_by_time_range": search_by_time_range,
    }
    
    outputs = []
    last_message = state["messages"][-1]
    
    for tool_call in getattr(last_message, "tool_calls", []):
        tool = tools_by_name[tool_call["name"]]
        tool_args = tool_call["args"]
        
        # 注入 user_id
        if "user_id" not in tool_args:
            tool_args["user_id"] = state["user_id"]
        
        result = await tool.ainvoke(tool_args)
        outputs.append(ToolMessage(
            content=str(result),
            name=tool_call["name"],
            tool_call_id=tool_call["id"],
        ))
    
    return {"messages": outputs}


def should_continue_event_agent(state: EventRelationAgentState):
    """判断是否继续调用工具"""
    last_message = state["messages"][-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "continue"
    return "end"


# ============================================================
# 事件关联 Agent 的主入口
# ============================================================

async def find_relations_event_node(state: MemoProcessState) -> dict:
    """
    事件的关联查找主节点。
    创建一个临时的 ReAct Agent，让它自主搜索并决策关联关系。
    """
    # 先对事件做深度分析
    llm = ChatOpenAI(model="gpt-4o-mini")
    structured_llm = llm.with_structured_output(EventAnalysis)
    
    analysis = await structured_llm.ainvoke(f"""
    分析以下事件，提取其特征：

    标题：{state['title']}
    内容：{state['content']}
    
    请分析事件的类型、关键词、时间范围、优先级和周期性。
    """)
    
    # 构建事件关联 Agent
    workflow = StateGraph(EventRelationAgentState)
    workflow.add_node("agent", event_agent_call_model)
    workflow.add_node("tools", event_agent_tool_node)
    
    workflow.set_entry_point("agent")
    workflow.add_conditional_edges(
        "agent", should_continue_event_agent,
        {"continue": "tools", "end": END}
    )
    workflow.add_edge("tools", "agent")
    
    event_agent = workflow.compile()
    
    # 运行 Agent
    initial_state = EventRelationAgentState(
        messages=[
            HumanMessage(content=f"""
用户刚创建了一个新事件，请帮他找出所有关联的速记和事件。

事件详情：
- ID: {state['memo_id']}
- 标题：{state['title']}
- 描述：{state['content']}
- 创建时间：{state.get('created_at', 'N/A')}

你的任务：
1. 使用多个搜索工具，全面查找可能相关的内容
2. 对找到的内容进行关联度评估
3. 输出最终的关联决策列表
""")
        ],
        user_id=state["user_id"],
        event_id=state["memo_id"],
        event_data={
            "title": state["title"],
            "content": state["content"]
        },
        event_analysis=analysis,
        collected_candidates=[],
        final_decisions=[]
    )
    
    final_state = None
    async for event_state in event_agent.astream(initial_state):
        final_state = event_state
    
    # 解析 Agent 最终输出，提取关联决策
    final_message = final_state["messages"][-1]
    final_relations = []
    
    if hasattr(final_message, "content"):
        # 从 AI 响应中提取 JSON 列表（需要配合 prompt 约束）
        content = final_message.content
        # 这里实际中需要更健壮的解析逻辑，比如让 LLM 用 <DECISIONS> 标记包裹结果
        if "```json" in content and "```" in content:
            json_str = content.split("```json")[1].split("```")[0].strip()
            final_relations = json.loads(json_str)
        else:
            # 简化：解析行内 JSON
            import re
            json_pattern = r'\{[^{}]*\}'
            matches = re.findall(json_pattern, content)
            for match in matches:
                try:
                    final_relations.append(json.loads(match))
                except:
                    pass
    
    return {
        "relation_candidates": final_relations,
        "final_relations": final_relations,
    }


# ============================================================
# 统一的关联查找入口
# ============================================================

async def find_relations_node(state: MemoProcessState) -> dict:
    """
    统一的关联查找入口，根据 memo_type 路由到不同的查找策略。
    """
    if state["memo_type"] == "event":
        return await find_relations_event_node(state)
    else:
        return await find_relations_quicknote(state)
```

#### 节点 5：关联判定

速记找到候选后，需要 LLM 判定是否真的关联、关联类型、关联强度。

```python
# memo_agent/nodes/judge_relations.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from memo_agent.schemas import RelationBatchResult
from memo_agent.state import MemoProcessState


async def judge_relations_node(state: MemoProcessState) -> dict:
    """
    对候选的关联内容进行批量判定，决定是否建立真实的关联关系。
    """
    if not state["relation_candidates"]:
        return {"final_relations": []}
    
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    structured_llm = llm.with_structured_output(RelationBatchResult)
    
    content = state["content"]
    candidates_json = json.dumps(state["relation_candidates"], ensure_ascii=False, indent=2)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """你是一个内容关联专家。用户有一条新速记，系统找到了一些可能相关的内容。

你的任务：判断这些候选内容是否真的与新速记相关，如果相关则确定关系类型和强度。

关系类型说明：
- RELATED_TO: 一般性关联，语义上有相关性但没有强因果关系
- LINKED_TO: 直接链接，新内容是对候选内容的补充或延续
- EXTENDS: 扩展关系，新内容在候选内容的基础上展开了讨论
- CONTRADICTS: 矛盾关系，新内容与候选内容观点相左
- CAUSED_BY: 因果关系，新内容是由候选内容导致的

关联强度评分（0-1）：
- 0.9-1.0: 极强关联，必须关联
- 0.7-0.9: 强关联，应该关联
- 0.5-0.7: 中等关联，可选关联
- 0.3-0.5: 弱关联，不建议关联
- 0.0-0.3: 无关，不关联

规则：
1. 只建立评分 > 0.5 的关联
2. 关联理由要清晰具体
3. 如果有多个候选都相关，按评分排序保留前 5 个"""),
        ("human", """新速记内容：
{content}

候选相关内容：
{candidates}

请判断每个候选是否应该关联，并给出详细的关系类型、评分和理由。""")
    ])

    chain = prompt | structured_llm
    result = await chain.ainvoke({
        "content": content,
        "candidates": candidates_json,
    })
    
    # 过滤掉 should_link=False 的结果
    final_relations = [
        j.model_dump() for j in result.judgments if j.should_link
    ]
    
    return {
        "final_relations": final_relations,
    }
```

#### 节点 6：事件绑定判定

速记还需要判断是否应该绑定到某个/某些事件上。

```python
# memo_agent/nodes/bind_events.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from memo_agent.schemas import EventBindingBatchResult
from memo_agent.state import MemoProcessState


async def bind_events_node(state: MemoProcessState) -> dict:
    """
    判断当前速记应该绑定到哪些事件上。
    事件绑定是速记与事件的特殊关联关系，用于"事件看板"视图。
    """
    ctx = state["user_graph_context"]
    active_events = ctx["active_events"]
    
    if not active_events:
        return {"event_links": []}
    
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    structured_llm = llm.with_structured_output(EventBindingBatchResult)
    
    events_json = json.dumps(active_events, ensure_ascii=False, indent=2)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """你是一个事件关联专家。用户有一条新速记，请判断它应该绑定到哪些活跃事件上。

事件绑定规则：
1. 如果速记的内容与某个事件的目标、进展、相关讨论直接相关，应该绑定
2. 如果速记是事件的执行过程记录，必须绑定
3. 如果速记只是偶尔提及事件但不涉及具体内容，不绑定

绑定强度评分（0-1）：
- > 0.7: 强绑定，应该绑定
- 0.5-0.7: 中等绑定，可选绑定  
- < 0.5: 弱绑定，不绑定

输出：
- decisions: 对每个活跃事件的绑定决策
- auto_detected_events: 如果内容中提及了用户尚未创建的事件，列出建议"""),
        ("human", """新速记内容：
{content}

用户当前活跃的事件列表：
{events}

请判断该速记应该绑定到哪些事件，以及是否检测到潜在的新事件。""")
    ])

    chain = prompt | structured_llm
    result = await chain.ainvoke({
        "content": state["content"],
        "events": events_json,
    })
    
    # 只保留 should_bind=True 的绑定
    final_links = [
        d.model_dump() for d in result.decisions if d.should_bind
    ]
    
    return {
        "event_links": final_links,
    }
```

#### 节点 7：写入 Neo4j

所有 Agent 决策完成后，批量写入 Neo4j。

```python
# memo_agent/nodes/persist_graph.py
import json
from neo4j import AsyncGraphDatabase

from memo_agent.state import MemoProcessState

NEO4J_URI = "bolt://localhost:7687"
NEO4J_AUTH = ("neo4j", "password")


async def persist_graph_node(state: MemoProcessState) -> dict:
    """
    将所有处理结果写入 Neo4j 知识图谱。
    包括：Memo/Event 节点、分类关系、标签关系、实体节点、关联关系、事件绑定。
    """
    user_id = state["user_id"]
    memo_id = state["memo_id"]
    memo_type = state["memo_type"]
    title = state["title"]
    content = state["content"]
    
    classification = state["classification_result"]
    extraction = state["extraction_result"]
    relations = state["final_relations"]
    event_links = state["event_links"]
    
    async with AsyncGraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH) as driver:
        async with driver.session() as session:
            # 使用事务批量执行
            async with session.begin_transaction() as tx:
                # 1. 创建 Memo/Event 节点
                if memo_type == "event":
                    await tx.run("""
                        MERGE (u:User {user_id: $user_id})
                        MERGE (e:Event {event_id: $id})
                        ON CREATE SET e.title = $title, e.description = $content,
                                       e.event_type = $event_type,
                                       e.status = 'active',
                                       e.created_at = datetime()
                        ON MATCH SET e.updated_at = datetime()
                        MERGE (u)-[:OWNS]->(e)
                    """, user_id=user_id, id=memo_id, title=title,
                        content=content, event_type=extraction.get("event_type", "general"))
                else:
                    await tx.run("""
                        MERGE (u:User {user_id: $user_id})
                        MERGE (m:Memo {memo_id: $id})
                        ON CREATE SET m.title = $title, m.content = $content,
                                       m.type = 'quick_note',
                                       m.created_at = datetime()
                        ON MATCH SET m.updated_at = datetime()
                        MERGE (u)-[:OWNS]->(m)
                    """, user_id=user_id, id=memo_id, title=title, content=content)
                
                # 2. 建立分类关系
                primary_cat = classification["primary_category"]
                secondary_cat = classification["secondary_category"]
                
                await tx.run("""
                    MATCH (u:User {user_id: $uid})-[:OWNS]->(x)
                    WHERE x.memo_id = $id OR x.event_id = $id
                    MERGE (c1:Category {name: $primary})
                    MERGE (c2:Category {name: $secondary})
                    MERGE (c2)-[:CHILD_OF]->(c1)
                    MERGE (x)-[:BELONGS_TO]->(c2)
                """, uid=user_id, id=memo_id, primary=primary_cat, secondary=secondary_cat)
                
                # 3. 创建/关联标签
                for tag in extraction["tags"]:
                    tag_name = tag["name"]
                    await tx.run("""
                        MATCH (u:User {user_id: $uid})-[:OWNS]->(x)
                        WHERE x.memo_id = $id OR x.event_id = $id
                        MERGE (t:Tag {name: $name})
                        MERGE (x)-[:HAS_TAG]->(t)
                    """, uid=user_id, id=memo_id, name=tag_name)
                
                # 4. 创建/关联实体
                for entity in extraction["entities"]:
                    entity_name = entity["name"]
                    entity_type = entity["entity_type"]
                    await tx.run("""
                        MATCH (u:User {user_id: $uid})-[:OWNS]->(x)
                        WHERE x.memo_id = $id OR x.event_id = $id
                        MERGE (en:Entity {name: $name, type: $type})
                        ON CREATE SET en.created_at = datetime()
                        MERGE (x)-[:MENTIONS]->(en)
                    """, uid=user_id, id=memo_id, name=entity_name, type=entity_type)
                
                # 5. 创建关联关系（速记-速记 / 事件-事件）
                for rel in relations:
                    target_id = rel["target_id"]
                    rel_type = rel["relation_type"]
                    score = rel["score"]
                    reason = rel["reason"]
                    
                    await tx.run("""
                        MATCH (u:User {user_id: $uid})
                        MATCH (s) WHERE (s.memo_id = $src_id OR s.event_id = $src_id)
                        MATCH (t) WHERE (t.memo_id = $tgt_id OR t.event_id = $tgt_id)
                        CALL apoc.create.relationship(s, $rel_type, {
                            score: $score, 
                            reason: $reason,
                            created_at: datetime()
                        }, t) YIELD rel
                    """, uid=user_id, src_id=memo_id, tgt_id=target_id,
                        rel_type=rel_type, score=score, reason=reason)
                
                # 6. 创建事件绑定关系（仅速记）
                if memo_type == "quick_note" and event_links:
                    for link in event_links:
                        event_id = link["event_id"]
                        binding_strength = link["binding_strength"]
                        reason = link["binding_reason"]
                        
                        await tx.run("""
                            MATCH (u:User {user_id: $uid})
                            MATCH (m:Memo {memo_id: $memo_id})
                            MATCH (e:Event {event_id: $event_id})
                            MERGE (m)-[:LINKED_TO {
                                strength: $strength,
                                reason: $reason,
                                created_at: datetime()
                            }]->(e)
                        """, uid=user_id, memo_id=memo_id, event_id=event_id,
                            strength=binding_strength, reason=reason)
                
                await tx.commit()
    
    return {}
```

### 3.4 LangGraph 主工作流组装

```python
# memo_agent/workflow.py
from langgraph.graph import StateGraph, END

from memo_agent.nodes.load_context import load_user_graph_context
from memo_agent.nodes.classify import classify_node
from memo_agent.nodes.extract import extract_tags_entities_node
from memo_agent.nodes.find_relations import find_relations_node
from memo_agent.nodes.judge_relations import judge_relations_node
from memo_agent.nodes.bind_events import bind_events_node
from memo_agent.nodes.persist_graph import persist_graph_node
from memo_agent.state import MemoProcessState


def create_memo_processing_graph():
    """
    构建速记处理的主工作流。

    流程：
    1. load_context: 加载用户图谱上下文
    2. classify: 匹配分类
    3. extract: 提取标签和实体
    4. find_relations: 查找相关内容（速记=被动匹配，事件=ReAct Agent 主动搜索）
    5. judge_relations: 判定关联关系（仅速记需要，事件由 Agent 内部完成）
    6. bind_events: 绑定事件（仅速记需要）
    7. persist_graph: 写入 Neo4j
    """
    workflow = StateGraph(MemoProcessState)
    
    # 添加所有节点
    workflow.add_node("load_context", load_user_graph_context)
    workflow.add_node("classify", classify_node)
    workflow.add_node("extract", extract_tags_entities_node)
    workflow.add_node("find_relations", find_relations_node)
    workflow.add_node("judge_relations", judge_relations_node)
    workflow.add_node("bind_events", bind_events_node)
    workflow.add_node("persist_graph", persist_graph_node)
    
    # 设置入口
    workflow.set_entry_point("load_context")
    
    # 标准路径：
    workflow.add_edge("load_context", "classify")
    workflow.add_edge("classify", "extract")
    
    # 从 extract 开始分流
    workflow.add_edge("extract", "find_relations")
    
    # find_relations 后分流
    # 对于速记：需要 judge_relations 和 bind_events
    # 对于事件：Agent 内部已完成所有判定，直接 persist
    
    def route_after_find_relations(state: MemoProcessState):
        """根据 memo_type 决定后续流程"""
        if state["memo_type"] == "event":
            return "event_path"
        else:
            return "quicknote_path"
    
    workflow.add_conditional_edges(
        "find_relations",
        route_after_find_relations,
        {
            "quicknote_path": "judge_relations",
            "event_path": "persist_graph",
        }
    )
    
    # 速记路径：judge_relations -> bind_events -> persist_graph
    workflow.add_edge("judge_relations", "bind_events")
    workflow.add_edge("bind_events", "persist_graph")
    
    # 结束
    workflow.add_edge("persist_graph", END)
    
    return workflow.compile()


# ============================================================
# 使用示例
# ============================================================

async def process_new_memo(user_id: int, memo_type: str, title: str, content: str):
    """处理新的速记或事件"""
    from sqlalchemy import select
    from backend.models import Memo  # SQLAlchemy 模型
    
    # 1. 先写入 MySQL，获取 memo_id
    async with db_session() as session:
        memo = Memo(
            user_id=user_id,
            type=memo_type,
            title=title,
            content=content,
            status="active"
        )
        session.add(memo)
        await session.commit()
        await session.refresh(memo)
        memo_id = memo.id
    
    # 2. 运行 LangGraph 工作流
    graph = create_memo_processing_graph()
    
    initial_state = MemoProcessState(
        messages=[],
        user_id=user_id,
        memo_id=memo_id,
        memo_type=memo_type,
        title=title,
        content=content,
    )
    
    final_state = None
    async for state in graph.astream(initial_state):
        final_state = state
    
    # 3. 更新 MySQL 中的处理状态
    async with db_session() as session:
        memo = await session.get(Memo, memo_id)
        memo.processed = True
        await session.commit()
    
    return {
        "memo_id": memo_id,
        "classification": final_state.get("classification_result"),
        "extraction": final_state.get("extraction_result"),
        "relations": final_state.get("final_relations"),
        "event_links": final_state.get("event_links"),
    }
```

## 四、FastAPI 接口层

```python
# backend/api/v1/memos.py
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from pydantic import BaseModel
from sqlalchemy.ext.asyncio import AsyncSession

from backend.core.deps import get_current_user, get_db
from backend.models import User
from memo_agent.workflow import process_new_memo

router = APIRouter(prefix="/memos", tags=["memos"])


class CreateMemoRequest(BaseModel):
    title: str
    content: str
    type: str = "quick_note"  # quick_note | event


class CreateMemoResponse(BaseModel):
    memo_id: int
    status: str


@router.post("/", response_model=CreateMemoResponse)
async def create_memo(
    request: CreateMemoRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    创建一条新的速记或事件。
    
    对于速记（quick_note）：同步处理，等待 1-3 秒返回
    对于事件（event）：异步处理，立刻返回，后台 Celery 执行
    """
    user_id = current_user.id
    
    if request.type == "event":
        # 事件：放入 Celery 队列异步处理
        from backend.tasks import process_event_task
        process_event_task.delay(
            user_id=user_id,
            title=request.title,
            content=request.content,
        )
        return {"memo_id": 0, "status": "processing"}
    else:
        # 速记：同步处理
        result = await process_new_memo(
            user_id=user_id,
            memo_type=request.type,
            title=request.title,
            content=request.content,
        )
        return {"memo_id": result["memo_id"], "status": "completed"}


@router.post("/audio")
async def create_memo_from_audio(
    audio_file: UploadFile,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    上传音频，自动转文字后创建速记。
    """
    # 1. 保存音频文件
    import uuid
    filename = f"{uuid.uuid4()}.webm"
    audio_path = f"/data/audio/{filename}"
    with open(audio_path, "wb") as f:
        f.write(await audio_file.read())
    
    # 2. 调用 Whisper 转文字
    from faster_whisper import WhisperModel
    model = WhisperModel("large-v3", device="cuda", compute_type="float16")
    
    segments, info = model.transcribe(audio_path, language="zh")
    text = "".join([segment.text for segment in segments])
    
    # 3. 创建速记
    result = await process_new_memo(
        user_id=current_user.id,
        memo_type="quick_note",
        title=f"语音笔记 {info.duration:.1f}s",
        content=text,
    )
    
    # 4. 更新音频 URL
    async with db:
        memo = await db.get(Memo, result["memo_id"])
        memo.audio_url = f"/audio/{filename}"
        await db.commit()
    
    return {"memo_id": result["memo_id"], "content": text}
```

## 五、Celery 异步任务（用于事件处理）

```python
# backend/tasks.py
from celery import Celery

celery_app = Celery('quickmemo', broker='redis://localhost:6379/0')


@celery_app.task
def process_event_task(user_id: int, title: str, content: str):
    """
    异步处理事件创建。
    事件处理需要较长时间（全量检索用户内容），放入 Celery 后台执行。
    """
    import asyncio
    from memo_agent.workflow import process_new_memo
    
    result = asyncio.run(process_new_memo(
        user_id=user_id,
        memo_type="event",
        title=title,
        content=content,
    ))
    
    return result
```

## 六、关键设计决策说明

### 6.1 为什么速记和事件处理流程不同？

**根本原因**：内容性质和关联方向不同。

速记是"碎片化信息"，它的价值在于被组织到已有的知识结构中。所以策略是"被动匹配"——查询现有图谱结构，找到应该挂载的位置。

事件是"结构性信息"，它的价值在于连接分散的内容。所以策略是"主动搜索"——用 Agent 全面扫描，主动建立关联网络。

### 6.2 事件关联 Agent 为什么用 ReAct 而不是固定工具链？

固定工具链（如：先搜向量、再搜标签、再搜实体）的缺点是：

1. 无法根据事件类型动态调整搜索策略
   - 项目类事件应该多搜时间范围和标签
   - 习惯类事件应该多搜时间规律
   - 性格标记类事件应该多搜实体和语义相似度

2. 无法自适应地优化搜索结果
   - 第一次搜索结果可能不理想，需要根据结果调整搜索词重新搜索
   - 固定工具链做不到这种反馈循环

ReAct Agent 的优势：自主决策调用哪些工具、调用顺序、调用参数，还能根据工具返回结果调整策略。这是"智能搜索"的关键。

### 6.3 为什么分类节点要先查询用户现有分类？

这是"分类 vs 标签"的本质区别：

- **分类**：是层级化的、全局的、相对固定的。用户建立了一个分类体系，所有内容都应该归入这个体系。所以必须先查询用户有哪些分类，让 LLM 从中选择。
- **标签**：是扁平化的、局部的、动态增长的。标签可以自由创建，但优先复用已有标签。

### 6.4 Neo4j 写入为什么用 APOC？

APOC 的 `apoc.create.relationship` 可以动态指定关系类型。

普通 Cypher 的关系类型必须是编译时确定的常量，不能从变量读取：

```cypher
-- 这是错误的，关系类型不能是变量
MATCH (a)-[r: $reltype]->(b)  -- ❌

-- APOC 方式（正确）
CALL apoc.create.relationship(a, $reltype, {prop: $val}, b)  -- ✅
```

在我们的场景中，关系类型（RELATED_TO/LINKED_TO/EXTENDS 等）是由 LLM 动态决定的，所以必须用 APOC。

## 七、性能与成本优化

### 7.1 Embedding 缓存

速记内容的 embedding 可能会重复生成（比如同一句话在不同工具调用中被多次 embed）。用 Redis 缓存：

```python
async def get_embedding(text: str) -> list[float]:
    cache_key = f"emb:hash:{hashlib.md5(text.encode()).hexdigest()}"
    cached = await redis.get(cache_key)
    if cached:
        return json.loads(cached)
    
    embedding = await embeddings.aembed_query(text)
    await redis.setex(cache_key, 86400 * 7, json.dumps(embedding))  # 缓存 7 天
    return embedding
```

### 7.2 Agent 调用次数限制

防止 Agent 陷入无限循环，设置最大工具调用次数：

```python
async def run_agent_with_limit(agent, initial_state, max_steps: int = 10):
    """运行 Agent，限制最大步骤数"""
    step_count = 0
    final_state = initial_state
    
    async for state in agent.astream(final_state):
        step_count += 1
        final_state = state
        if step_count >= max_steps:
            break
    
    return final_state
```

### 7.3 批量写入

所有 Neo4j 写入操作都在单个事务中完成，避免多次事务开销。

### 7.4 LLM 模型选择

- 分类节点、提取节点：使用 `gpt-4o-mini`（便宜、快）
- 事件关联 Agent：使用 `gpt-4o`（需要更强的推理能力）
- 关联判定节点：使用 `gpt-4o-mini`

成本估算（每条速记）：
- 分类：~10 tokens
- 提取：~50 tokens
- 关联判定（如果有 5 个候选）：~30 tokens
- 总计：~90 tokens × $0.15/1M tokens = $0.0000135 ≈ 0.01 分人民币

## 八、测试策略

### 8.1 单元测试

```python
# tests/test_memo_agent.py
import pytest
from memo_agent.workflow import create_memo_processing_graph
from memo_agent.state import MemoProcessState


@pytest.mark.asyncio
async def test_quick_note_processing():
    """测试速记处理流程"""
    graph = create_memo_processing_graph()
    
    initial_state = MemoProcessState(
        messages=[],
        user_id=1,
        memo_id=1001,
        memo_type="quick_note",
        title="测试速记",
        content="这是一个测试速记，包含标签 #Python 和实体 LangChain。",
    )
    
    final_state = None
    async for state in graph.astream(initial_state):
        final_state = state
    
    assert final_state["classification_result"] is not None
    assert final_state["extraction_result"]["tags"] is not None
    assert final_state["final_relations"] is not None


@pytest.mark.asyncio  
async def test_event_processing():
    """测试事件处理流程"""
    graph = create_memo_processing_graph()
    
    initial_state = MemoProcessState(
        messages=[],
        user_id=1,
        memo_id=2001,
        memo_type="event",
        title="完成知识图谱项目",
        content="Q1 完成 LangChain+Neo4j 知识图谱系统上线，包括速记、分类、搜索功能。",
    )
    
    final_state = None
    async for state in graph.astream(initial_state):
        final_state = state
    
    assert len(final_state["final_relations"]) >= 0
```

### 8.2 集成测试

使用 Docker Compose 启动完整环境（MySQL、Neo4j、Redis、Celery），然后跑端到端测试。

## 九、下一步开发建议

1. **先实现速记流程**（被动匹配）：load_context → classify → extract → find_relations_quicknote → judge_relations → bind_events → persist
2. **验证基本功能**：分类正确、标签提取可靠、关联关系合理
3. **再实现事件流程**（ReAct Agent）：find_relations_event_node 中的 4 个工具和 Agent 逻辑
4. **优化 Agent prompt**：根据实际效果调整 prompt，让搜索策略更智能
5. **性能调优**：监控各节点耗时，优化瓶颈（通常是 Neo4j 查询和 LLM 调用）
6. **前端对接**：React 调用 FastAPI 接口，实现实时反馈

---

文档版本：v1.0
最后更新：2026-02-14
依赖版本：langchain==1.2.7, langchain-core==1.2.0, langgraph==0.4.x
